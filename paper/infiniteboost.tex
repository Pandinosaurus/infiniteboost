\begin{abstract}
    Ensemble methods have demonstrated high accuracy for a variety of problems in different areas of machine learning.
% FIXME
    The most known algorithms intensively used in practice are random forests and gradient boosting.
    In this paper we present InfiniteBoost --- a novel algorithm, which combines the best properties of these two approaches. 
    The algorithm constructs an ensemble of trees for which the following two properties hold: trees of the ensemble account for mistakes of each other and at the same time the ensemble may contain an infinite number of trees without over-fitting.
    The proposed algorithm is evaluated on regression, classification, and ranking tasks using large scale, publicly available datasets.
    % + строит ансамбль, где деревья учитывают ошибки других, и может быть бесконечным без переобучения
\end{abstract}

\section{Introduction}
% + XGBoost + kaggle
% + 1 - на кдд16 yahoo рассказал, что их поиск работает по сути на градиентном бустинге, обученном на логлосс и вот статья http://www.yichang-cs.com/yahoo/KDD16_yahoosearch.pdf

% FIXME
Nowadays ensemble methods of machine learning are one of the most widespread approaches used in many applications in industry and science:
search engines, recommendations, store sales prediction, customer behavior prediction, web text classification, high energy physics, astronomy, computational biology and chemistry, etc.
They achieve state-of-the-art results not only on many standard benchmarks~\cite{key-benchmarks,key-benchmarks-forest} but also in real-world tasks, like movements of individual body parts prediction~\cite{key-kinect}, click through rate prediction~\cite{key-facebook}, ranking relevance prediction in search engines~\cite{key-yahoo}. 
Wide applicability of ensembles is confirmed by data challenges: 60\% of winning solutions of challenges on Kaggle used XGBoost~\cite{key-xgboost}, one of the popular gradient boosting implementations. 

In ensemble methods multiple individual predictors are trained for the same problem and after that are combined in some manner.
Predictors in the ensemble can be constructed independently, like in bagging~\cite{key-bagging} and, in particular, random forests~\cite{key-random-forest}, or depending upon the performance of the previous models, like in gradient boosting~\cite{key-gb}.

% In parallel, independent construction of predictors in the ensemble is developed. 
% L. Breiman proposes bagging predictors~\cite{key-bagging}, a method for generating multiple versions of a predictor and using these to get an aggregated predictor. 
% The multiple versions are formed by making bootstrap replicates of the training set and using these as new training sets. 
% The aggregation is very simple: it averages over the versions.
Random forests suggested by L. Breiman~\cite{key-random-forest} are a combination of tree predictors such that each tree is constructed using the random subset of features and bagging~\cite{key-bagging} providing bootstrap replicas of the training set.
Tree predictors are aggregated by averaging.
It can be shown that the generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. 
As a result, one can include arbitrarily many trees in the ensemble without decreasing its performance on the unseen data. % поэтому можно сколько угодно деревьев на практике - и качество не ухудшается
Also L. Breiman shows that predictors in the ensemble should be accurate but uncorrelated to achieve better quality than the individual predictors. 
In practice accuracy of predictors is provided by very deep trees and low correlation is achieved by bagging and random features subsampling.
% опэтому на практике используются глубокие деревья, а их нескоррелированность - бэггингом и подфичеами 

In contrast to random forests, J. Friedman develops a general gradient descent boosting paradigm~\cite{key-gb,key-sgb}, an ensemble method of boosted regression trees.
It does the greedy function approximation with gradient descent method in the space of functions. 
On each iteration of boosting a new tree is constructed to approximate the gradient of the loss function with respect to the decision function. 
% FIXME
Such procedure allows provides accounting the performance and mistakes of previously constructed trees in the ensemble.
In real world applications gradient boosting models often have to contain thousands of trees to provide the best possible quality.
However, building arbitrarily large boosting models drives to decrease of quality on the unseen data~\cite{key-brownboost, key-dart}, an effect known as over-fitting.
% учитывает ошибки или качество предыдущих моделей

% FIXME
As soon as random forest and gradient boosting are widely used in practice and have own advantages and disadvantages we present a novel algorithm, called InfiniteBoost, the goal of which is to combine the best properties of both: allow for construction of an infinite (arbitrarily large) ensemble and at the same time account for the mistakes of previously constructed trees in the ensemble using a gradient descent method.

The remainder of the paper is organized as follows: in Section~2 we explain the motivation behind InfiniteBoost; in Section~3 we describe InfiniteBoost algorithm and prove the convergence theorem.
Comparison of the proposed algorithm with gradient boosting and random forests is given in Section~4: experimental results are listed for regression, classification, and ranking problems. 
Finally, we provide an overview of other existing boosting-bagging hybrid algorithms (term introduced by J. Friedman~\cite{key-sgb}) in Section~5.


\section{Motivation}
%FIXME
Here and further let's consider the ensembles over decision trees.
Random forests construct trees in the ensemble independently using randomness in feature selection and bagging to provide own training set for each predictor.
This independence makes it possible to build infinite ensembles: quality on the unseen data converges to some constant as the number of trees in the ensemble becomes large.

In contrast to random forests, gradient boosting procedure allows next tree to account for the mistakes done by the previous trees in the ensemble.
However, it is known that gradient boosting is prone to over-fitting.
In practice over-fitting can be detected using the learning curve when quality on the holdout increases depending on the iteration of boosting and then decreases starting from some iteration. 
% In this paper we refer to this situation as "over-fitting".
Thus, for gradient boosting an arbitrarily large number of predictors in an ensemble is not an option. 
%FIXME (maybe move up, also phrasing)
This over-fitting issue is considered as overspecialization in~\cite{key-dart} and described as the following: wherein trees added at later iterations tend to impact the prediction of only a few samples, and make negligible contribution towards the remaining samples.
To avoid over-fitting a shrinkage parameter is introduced to the boosting procedure: a new tree is added to the ensemble with a coefficient, called shrinkage (or learning rate).
Shrinkage is an important hyperparameter of gradient boosting which should be tuned in practice.

InfiniteBoost is an algorithm which aims to build an infinite ensemble (without over-fitting effect with increasing the number of predictors) such that each new predictor incorporates the errors made by the previous predictors in the ensemble (boosting procedure).

\section{InfiniteBoost}
We start our study from analyzing the desired properties of an algorithm:
a) build an infinite converging ensemble b) account for the errors made by trees using gradient boosting approach. Firstly, let us consider the gradient boosting algorithm (Algorithm~\ref{alg:gb}),
where the ensemble prediction $F(x)$ accumulates contribution of the trees $ F(x) = \eta \sum_{m=1}^M \text{tree}_m(x) $.

\begin{algorithm}[!h]
  \caption{Gradient boosting}\label{alg:gb}
  {\bf Input}: training set $\{x_i, y_i\}_{i=1}^n$; a differentiable loss function $L(y, F(x))$; number of boosting iterations $M$; shrinkage~$\eta$.

  {\bf Algorithm} ($F(x)$ is output):
  \begin{algorithmic}
    \State Initialize model with a zero value: $F(x) \gets 0$.
    \For{$m=1,\dots,M$}
      \State $\text{tree}_m \gets \text{learn}\left(\left\{x_i, -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right\}_{i=1}^n\right)$
      \State $F(x) \gets F(x) + \eta \text{ tree}_m(x)$
    \EndFor
  \end{algorithmic}

  Optional steps like finding initial constant prediction, usage of Newton-Raphson step,
  or usage of Hessian in the tree construction are skipped for brevity.
\end{algorithm}

To achieve a stationary state predictions in InfiniteBoost are averaged with weights $\alpha_m$
(that can be taken uniform $\alpha_m = 1$)
and scaled by an additional constant $c$, called ensemble capacity or simply capacity:
$ F(x) = c \times \dfrac{\sum_m \alpha_m \text{tree}_m(x)}{ \sum_m \alpha_m } $.
The contribution of each individual tree in this model converges to zero.
%FIXME
The model of interest should after sufficiently large amount of iterations converge to a stationary process
(we expect the process of tree building to be randomized, which is crucial in building powerful ensembles of trees),
and the distribution of newly-built trees should coincide with the distribution of trees already in the ensemble
\begin{equation*}
  F(x) = c \times \mathbb{E}_{\text{trees at }F(x)} \text{tree}(x)
\end{equation*}
with average taken over the trees generated by the usual training procedure
\formula{
    \text{trees at }F(x)= \\
    =\left\{\text{tree} \, \Big| \, \text{tree} \gets \text{learn}\left(\left\{x_i, -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right\}_{i=1}^n\right)\right\}.
}
The building process is determined by a vector $z$ of the current ensemble's predictions on the training set:
$ z = ( F(x_1), \dots, F(x_n) ) $.
Let $ \overline{T}(z)_i = \mathbb{E}_\text{tree at $z$} \text{tree}(x_i) $ be the average prediction of an ensemble on the training set. Then we can see that the stationary point of the process is a solution of an equation
\begin{equation}
    z = c \times \overline{T}(z) \label{eq:stationary}.
\end{equation}

\begin{theorem}
Equation~(\ref{eq:stationary}) has a solution if  
$\overline{T}(z)$ is bounded ($|| \overline{T}(z) || < \text{const}$) and continuous.
\end{theorem}
This follows directly from the Brouwer fixed-point theorem.
Boundedness holds if the gradient is bounded (as in logistic loss),
for other losses this can be achieved by scaling the gradient after some (large) threshold.
As for continuity of $\overline{T}(z)$,
this property can be enforced e.g. by adding Gaussian noise to $z$ before building a tree.

\begin{algorithm}[!h]
  \caption{Infinite Boosting (InifiniteBoost)}\label{alg:infiniteboost}
  {\bf Input}: training set $\{x_i, y_i\}_{i=1}^n$; a differentiable loss function $L(y, F(x))$; the number of boosting iterations $M$; capacity $c$.

  {\bf Algorithm} ($F(x)$ is output):
  \begin{algorithmic}
    \State Initialize model with a zero value: $F(x) \gets 0$.
    \For{$m=1,\dots,M$}
      \State $\text{tree}_m \gets \text{learn}\left(\left\{x_i, -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right\}_{i=1}^n\right)$
      \State $F(x) \gets  \frac{\sum_{k=1}^m \alpha_k \text{tree}_k(x)}{\sum_{k=1}^m \alpha_k} \times c$
    \EndFor
  \end{algorithmic}

  {\bf Remarks}:
  \begin{itemize}
    \item To avoid over-stepping in first iterations capacity $c$ is replaced with a value providing the contribution of a single tree not greater than one.
    \item To avoid recomputing ensemble ($\eta_m = \frac{\alpha_m}{\sum_{k=1}^m \alpha_k}$):
      \begin{equation*}
        F(x) \gets (1-\eta_m)F(x) + \eta_m \times c \times \text{tree}_m(x).
      \end{equation*}
  \end{itemize}
\end{algorithm}
A method we propose to find this stationary state is InfiniteBoost (Algorithm~\ref{alg:infiniteboost}).
It can be interpreted as a stochastic optimization of the following regularized loss function:
\begin{equation*}
  \dfrac{|| z ||^2}{2} + c \sum_{i=1}^n L(y_i, z_i) = \dfrac{|| z ||^2}{2} + c \mathcal{L},
\end{equation*}
% FIXME
while in stochastic gradient descent methods an update has the form
\formula{
    z \leftarrow z - \eta_m \left(z + c \times \text{grad}_z \mathcal{L} \right) = \\
    = (1 - \eta_m) z - \eta_m c \times \text{grad}_z \mathcal{L}.
}
As in the usual gradient boosting, the gradient step is replaced with building a tree modelling a negative gradient.
Then an ensemble prediction is updated similarly:
\begin{equation*}
    F(x) \leftarrow (1 - \eta_m) F(x) + \eta_m c \times \text{tree}_m(x).
\end{equation*}
Different choices of $\eta_m$ are possible and correspond to different weightings $\alpha_m$.
For example, $\eta_m = 1 / m$ correspond to uniform weighting $\alpha_m = 1$,
while $\eta_m = 2 / (m + 1)$ correspond to assigning higher weights for later trees $\alpha_m = m$.
In our experiments the second option is used, because it keeps the effective sample size of $\frac{3}{4}m$ (compatible with uniform weighting),
but allows ensemble to adapt faster to new values of $z$:
to obtain 99\% contribution of "new trees" to the ensemble one needs to enlarge size of the ensemble by a factor of 10,
whereas with uniform weighting the size should be enlarged by 100 times.

% TODO few words about adaptation (less number of tree for decreasing influence)
% TODO link on effective sample size

\begin{theorem}
InfiniteBoost converges almost surely to the solution of Equation (\ref{eq:stationary}),
provided that $c \times \overline{T}(z)$ is a contraction mapping and
$\eta_m \sim \frac{1}{m}$.
\end{theorem}

The contraction requirement may hold only for sufficiently small capacities $c$,
also it implies there is one and only one solution $z^{*}$ of Equation (\ref{eq:stationary}).
Theorem follows from an inequality
\formula{
    \mathbb{E} || z_{m+1} - z^{*} ||^2 \leq \\
    \leq \left(1 - \text{const}_1 \, \eta_m  \right)^2 \mathbb{E} || z_m - z^{*} ||^2
    + \text{const}_2 \, \eta^2_m
}
that holds for some positive constants $\text{const}_1$, $\text{const}_2$,
consequently, $\lim_{m \to +\infty} \mathbb{E} || z_{m} - z^{*} ||^2 = 0$.

% We empirically observed the convergence of an ensemble for sufficiently large capacities.
Capacity $c$ of an ensemble can be changed in the process of building,
allowing next trees to find another optimal point.
Since the loss is differentiable with respect to capacity $c$,
we present a variant of InfiniteBoost (Algorithm \ref{alg:infiniteboost_adaptive}) which adapts $c$ using holdout.
In experiments, 5\% of the training set is used as a holdout to tune capacity.
This way, one may skip the process of tuning capacity (or similar parameter shrinkage of gradient boosting).
After properly selecting capacity, holdout can be added to the training set, but this was not done in our experiments.

\begin{algorithm}[!h]
  \caption{Infinite Boosting with adaptive capacity (InifiniteBoost)}\label{alg:infiniteboost_adaptive}
  {\bf Input}: training set $\{x_i, y_i\}_{i=1}^n$; a differentiable loss function $L(y, F(x))$; the number of boosting iterations $M$.

  {\bf Algorithm} ($F(x)$ is output):
  \begin{algorithmic}
    \State Initialize: model $F(x) \gets 0$; capacity $c=\frac{1}{2}$.
    \State Divide training set into two non-overlapping subsets $\{x_i, y_i\}_{i=1}^n=T\sqcup H$.
    \For{$m=1,\dots,M$}
      \State $\text{tree}_m \gets \text{learn}\left(\left\{x_i, -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right\}_{\{x_i, y_i\}\in T}\right)$
      \State $F(x) \gets \frac{\sum_{k=1}^m \alpha_k \text{tree}_k(x)}{\sum_{k=1}^m \alpha_k} \times c$
      \State Use very minor correction of capacity:
      \State $s \gets \text{sgn} \sum_{\{x_i, y_i\}\in H} -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} F(x_i)$
      \State $c \gets c\times \left(\frac{m+1}{m}\right)^s$
    \EndFor
  \end{algorithmic}
\end{algorithm}

\section{Experiments}
We have evaluated InfiniteBoost for three different tasks: classification, regression, and ranking. 
%FIXME: repeated phrase abuse
Large scale, publicly available datasets are used in comparison. 
In our evaluation, we compare InfiniteBoost with two extreme cases: random forests and gradient boosting with different shrinkage values.
The source code with all experiments is available on the github: \url{https://github.com/arogozhnikov/infiniteboost}.

\insertplotsgbarxiv
\insertplotsforestarxiv

\subsection{InfiniteBoost and gradient boosting}
Datasets used for experiments are listed in Table~\ref{tab:gb-data}. 
For Higgs dataset 1 million samples are used for training and 500,000 samples are used for test. 
For YearPredictionMSD dataset random 75\% are taken as training and the remaining samples are taken for test purposes.
InifiniteBoost uses adaptive capacity for classification and regression problems.
For ranking task the fixed capacity value is used because the loss function is not convex in this case and the adaptation on the holdout significantly underestimates capacity.
For all tasks the same hyperparameters are used for gradient boosting and InfiniteBoost: subsample is set to 0.7, max features --- 0.7, max depth --- 7. 
For gradient boosting shrinkage is varied to compare with InfiniteBoost.
Other hyperparameters settings are tested and the similar behavior is observed.
In Figures~\ref{fig:gb-auc}, \ref{fig:gb-mse}, \ref{fig:gb-ndcg} the learning curves are presented.
It can be seen that InifiniteBoost provides similar quality as gradient boosting (with appropriate shrinkage) and at the same time it is free from the over-fitting effect. 
As the number of trees increases the quality of InfiniteBoost tends to some constant.
Notably, found optimal capacities differ significantly for trees of different complexity (Figure~\ref{fig:gb-depth}).

\insertplotsgbnips

\subsection{InfiniteBoost and random forest}
Random forests are known for their good quality with standard hyperparameters.
InfiniteBoost with small capacity almost does not encounter mistakes done at the previous stages,
thus, behaves similarly to random forest.

In benchmarks we use the implementation of random forest from scikit-learn~\cite{key-sklearn}
with default settings (without limiting depth) and datasets (see Table~\ref{tab:rf-data}) with known good performance of random forest.
To have side-by-side comparison InfiniteBoost uses deep trees with the same parameters, just as random forest.
Such deep trees over-fit to the training data, therefore, they are usually considered to be inappropriate for boosting.
In the experiments InfiniteBoost is tested with different fixed capacity values and it is observed that
adding a bit of boosting behavior to random forest by setting small capacity improves the model for 2 of 3 datasets (see Table~\ref{tab:rf-auc}).
Our new algorithm is capable of using the mistakes done by the previous trees on out-of-bag samples, what makes boosting possible.
In experiments when large number of trees was feasible, quality of InfiniteBoost converges to some constant
showing behavior similar to random forest (Figure~\ref{fig:inf}).

\insertplotsforestnips

\clearpagebiblio 
\section{Related works}
There are different attempts of combining properties of random forests and gradient boosting.
% The first successful in practice ensemble method is AdaBoost~\cite{key-adaboost}.
% The predictors in the ensemble are constructed consequently in the adaptive way: a next predictor is trained to pay more attention to samples misclassified by the previous predictors.
% AdaBoost performs well on a variety of datasets, however, it is sensitive to the noise in data and outliers due to focusing on misclassified samples.
% To prevent this adaptation to noise and outliers the BrownBoost~\cite{key-brownboost} is proposed.
% The core assumption of BrownBoost is that noisy samples will be repeatedly misclassified by the predictors and non-noisy samples will be correctly labeled frequently. 
% This allows to exclude noisy samples and provide only non-noisy examples contribution to the ensemble. 
% Thus, it is expected to obtain much better generalization error for the ensemble learned from non-noisy samples than for the ensemble learned from noisy and non-noisy samples. 
% The amount of error to be tolerated in the training set is a parameter of the BrownBoost.
%LPboost
Bagging methods are mainly proposed to effectively reduce the variance of regression predictors, while they leave bias relatively unchanged. 
To reduce both bias and variance iterated bagging was developed~\cite{key-iterated-bagging}. 
%FIXME?
It works in stages: on each stage bagging is trained and its outcomes are used to alter the target values; this is repeated until a simple rule stops the process. 
The idea behind bagging is to train each predictor in the ensemble iteratively on the difference between the target and the prediction of the ensemble constructed by this moment (also known as the residual).
Thus, iterated bagging includes the property of gradient boosting: each new predictor in the ensemble accounts for the performance of the previous predictors.
Bagging procedure and its out-of-bag estimation are used to obtain unbiased estimate of the “true” residual. 

Another approach is stochastic gradient boosting~\cite{key-sgb} that introduces randomness into gradient boosting by providing lower correlation between predictors like in random forests. 
On each boosting iteration subsample of the training data drawn at random (without replacement) from the full training set is used to fit a new predictor in the ensemble. 
The lower size of subsample the more random samples will differ and the more randomness will be introduced into boosting procedure.
At the same time lower size of subsample reduces the amount of data used in new predictor fitting.
This causes the variance of individual predictors to increase.
This version of gradient boosting is widely used in practice and gives good results.

As discussed above, over-fitting is a well-known problem for gradient boosting. 
The idea of another boosting-bagging hybrid algorithm~\cite{key-dart}, called DART, is to fight this issue by employing dropouts idea for ensembles of trees: muting complete trees as opposed to muting features in random forests. 
On each boosting iteration randomly chosen trees form a new ensemble~$M'$. 
A new regression tree is fitted to the negative gradient of the loss function with respect to the predictions obtained from the ensemble~$M'$.
Adding of a new tree to the initial ensemble is accompanied by a normalization step.
Firstly, the new tree is scaled by a factor $1/k$, where $k$ is the number of dropped trees from the initial ensemble, to provide the same order of magnitude for the new tree as the dropped trees. 
Secondly, the new tree and the dropped trees are scaled by a factor of $k/(k+1)$ and the new tree is added to the ensemble.
The last scaling ensures that the combined effect of the dropped trees together with the new tree remains the
same as the effect of the dropped trees alone before the introduction of the new tree.
InfiniteBoost and DART have similarities in the normalization step procedure, however, DART does not use correction constant (capacity), like InfiniteBoost.

DART implements gradient boosting, if no tree is dropped, and random forests, if all the trees are dropped.
With comparison to gradient boosting, this algorithm has no shrinkage hyperparameter, which is replaced by dropout rate, a fraction of trees muted on each iteration.
However, construction of large ensembles is not feasible in this algorithm from the computational point.
On each learning iteration it is needed to prepare new target by predicting the training set with a random subset of already constructed trees.
This leads to the quadratic (not linear as for gradient boosting) dependence between training time and the number of built trees.\footnote{
There is a mode of DART algorithm, called $\eta$-algorithm, with dropping only one tree from the ensemble on each iteration.
It is linear in time w.r.t. the number of trees in the ensemble.}
Also, the algorithm does not converge to some point for arbitrarily large number of trees because the contribution of newly-built tree does not tend to zero, which causes over-fitting.
% обучение нелинейно зависит от числа построенных деревьем (квадратично)
% + e-алгоритм описание и он линеен. На большом количестве деревьев асимптотически эквивалентен нашему.
% + процедура нормировки (перекликается с нашей), но нет корректирующей константы

Simple approach of combining random forests and boosting is applied in~\cite{key-bagboo}, called BagBoo. Each predictor in the bootstrap aggregated ensemble is gradient boosting. This approach aims to be well parallelized (each gradient boosting predictor contains 10-20 trees).

%FIXME
For drug discovery data having varying levels of noise composition of another properties of boosting and random forests is proposed. Ensemble Bridge~\cite{key-bridge} algorithm is capable of transitioning between boosting and random forests using a regularization parameter, which controls the tree depth, feature selection and randomness in the selection of samples for the tree construction. 

\section{Conclusion}
InfiniteBoost is a new hybrid algorithm. 
%FIXME repetition
On one side, it is random forest with trees in the ensemble incorporating the mistakes done by other trees.
On the other side, it is a modification of gradient boosting, which has no shrinkage hyperparameter to tune and does not over-fit with increasing the number of boosting iterations.
Empirically InfiniteBoost shows the quality not worse than random forests and asymptotically (with increasing the number of trees) not worse than gradient boosting trained with different shrinkage values.
This could save the time spent on shrinkage tuning for gradient boosting in applications.
Also, experiments demonstrate that learning curve on the unseen data for InfiniteBoost tends to the constant starting from some iteration of boosting (favorable property of random forests).
% As soon as random forest provides a lower error on the unseen data in the presence of a noisy response, while gradient boosting provides lower error on the unseen data in the presence of a clean response InfiniteBoost could work both for clean and noisy responses efficiently.
% построили форест, учитывающий ошибки предыдущих
% видоизменили градиентный бустинг, которые не имеет параметра (lr) + не переобучается с увеличением итераций
% Эмпирически показывает качество не хуже фореста и асимптотически (при большом числе деревьев) не хуже качества gb с различными lr.
\clearpagebiblio
\begin{thebibliography}{99}
\medskip

\small

\bibitem{key-bagging}
Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140.

\bibitem{key-random-forest}
Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.

\bibitem{key-iterated-bagging}
Breiman, L. (2001). Using iterated bagging to debias regressions. Machine Learning, 45(3), 261-277.

\bibitem{key-lambdamart}
Burges, C. J. (2010). From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581), 81.

\bibitem{key-benchmarks}
Caruana, R., \& Niculescu-Mizil, A. (2006, June). An empirical comparison of supervised learning algorithms. In Proceedings of the 23rd international conference on Machine learning (pp. 161-168). ACM.

\bibitem{key-yahoo-data}
Chapelle, O., \& Chang, Y. (2011, January). Yahoo! learning to rank challenge overview. In Yahoo! Learning to Rank Challenge (pp. 1-24).

\bibitem{key-xgboost}
Chen, T., \& Guestrin, C. (2016, August). Xgboost: A scalable tree boosting system. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785-794). ACM.

\bibitem{key-bridge}
Culp, M., Johnson, K., \& Michailidis, G. (2010). The ensemble bridge algorithm: A new modeling tool for drug discovery problems. Journal of chemical information and modeling, 50(2), 309-316.

\bibitem{key-benchmarks-forest}
Fernández-Delgado, M., Cernadas, E., Barro, S., \& Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems. J. Mach. Learn. Res, 15(1), 3133-3181.

\bibitem{key-brownboost}
Freund, Y. (2001). An adaptive version of the boost by majority algorithm. Machine learning, 43(3), 293-318.

\bibitem{key-adaboost}
Freund, Y., \& Schapire, R. E. (1995, March). A desicion-theoretic generalization of on-line learning and an application to boosting. In European conference on computational learning theory (pp. 23-37). Springer Berlin Heidelberg.

\bibitem{key-gb}
Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232.

\bibitem{key-sgb}
Friedman, J. H. (2002). Stochastic gradient boosting. Computational Statistics \& Data Analysis, 38(4), 367-378.

\bibitem{key-adaboost-boost}
Friedman, J., Hastie, T., \& Tibshirani, R. (2000). Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2), 337-407.

\bibitem{key-facebook}
He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., ... \& Candela, J. Q. (2014, August). Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising (pp. 1-9). ACM.

\bibitem{key-dart}
Korlakai Vinayak, R., \& Gilad-Bachrach, R. (2015). DART: Dropouts meet Multiple Additive Regression Trees. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (pp. 489-497).

\bibitem{key-bagboo}
Pavlov, D. Y., Gorodilov, A., \& Brunk, C. A. (2010, October). BagBoo: a scalable hybrid bagging-the-boosting model. In Proceedings of the 19th ACM international conference on Information and knowledge management (pp. 1897-1900). ACM.

\bibitem{key-sklearn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... \& Vanderplas, J. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12(Oct), 2825-2830.

\bibitem{key-kinect}
Shotton, J., Sharp, T., Kipman, A., Fitzgibbon, A., Finocchio, M., Blake, A., ... \& Moore, R. (2013). Real-time human pose recognition in parts from single depth images. Communications of the ACM, 56(1), 116-124.

\bibitem{key-yahoo}
Yin, D., Hu, Y., Tang, J., Daly, T., Zhou, M., Ouyang, H., ... \& Langlois, J. M. (2016, August). Ranking relevance in yahoo search. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 323-332). ACM.
\end{thebibliography}
